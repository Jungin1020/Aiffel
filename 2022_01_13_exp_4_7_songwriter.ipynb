{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022.01.13_exp_4-7_songwriter.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfF2dguVBfyc7IqxSQlPLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jungin1020/Aiffel_exp/blob/main/2022_01_13_exp_4_7_songwriter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibqz57FV-d_W",
        "outputId": "b5e12f5d-565e-418b-bab4-3001c69ee447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "데이터 크기 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import glob\n",
        "import os, re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/aiffel/lyricist/data/lyrics/*'\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "raw_corpus = []\n",
        "\n",
        "for txt_file in txt_list:\n",
        "  with open(txt_file,'r') as f:\n",
        "    raw = f.read().splitlines()\n",
        "    raw_corpus.extend(raw)\n",
        "\n",
        "print('데이터 크기', len(raw_corpus))\n",
        "print('Examples:\\n',raw_corpus[:3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx,sentence in enumerate(raw_corpus):\n",
        "  if len(sentence) == 0: continue\n",
        "  if idx > 9 : break\n",
        "\n",
        "  print(sentence)"
      ],
      "metadata": {
        "id": "CGr1r_Dp_1FW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9217b02f-d243-4bb6-cb70-4c55299b3632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for some education\n",
            "Made my way into the night\n",
            "All that bullshit conversation\n",
            "Baby, can't you read the signs? I won't bore you with the details, baby\n",
            "I don't even wanna waste your time\n",
            "Let's just say that maybe\n",
            "You could help me ease my mind\n",
            "I ain't Mr. Right But if you're looking for fast love\n",
            "If that's love in your eyes\n",
            "It's more than enough\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "  sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "  sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  sentence = '<start> ' + sentence + ' <end>'\n",
        "  return sentence\n",
        "\n",
        "\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ40p3xZBVJu",
        "outputId": "fa7ee58a-7ad8-4367-f46c-8846df69ab4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for sentence in raw_corpus:\n",
        "  if len(sentence) == 0: continue\n",
        "\n",
        "  preprocessed_sentence = preprocess_sentence(sentence)\n",
        "  corpus.append(preprocessed_sentence)\n",
        "\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLn6lXoJBzwV",
        "outputId": "a48c299b-eb31-4873-ecbf-4c9c32f60cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> looking for some education <end>',\n",
              " '<start> made my way into the night <end>',\n",
              " '<start> all that bullshit conversation <end>',\n",
              " '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>',\n",
              " '<start> i don t even wanna waste your time <end>',\n",
              " '<start> let s just say that maybe <end>',\n",
              " '<start> you could help me ease my mind <end>',\n",
              " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
              " '<start> if that s love in your eyes <end>',\n",
              " '<start> it s more than enough <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "  max_length = 15\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      num_words = 12000,\n",
        "      filters = '',\n",
        "      oov_token = '<unk>'\n",
        "  )\n",
        "\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  tensor = tokenizer.texts_to_sequences(corpus) #0 붙이기 전\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=max_length, padding='post')\n",
        "\n",
        "  print(tensor,tokenizer)\n",
        "  return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5fxzZJUCaP_",
        "outputId": "4f3334df-4a28-4738-967b-96380c605ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 306  28 ...   0   0   0]\n",
            " [  2 221  13 ...   0   0   0]\n",
            " [  2  24  17 ...   0   0   0]\n",
            " ...\n",
            " [  2   5  33 ...   0   0   0]\n",
            " [  2   5  33 ...  13 169   3]\n",
            " [  2   5 372 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f939ecf3d90>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2, 306,  28, ...,   0,   0,   0],\n",
              "       [  2, 221,  13, ...,   0,   0,   0],\n",
              "       [  2,  24,  17, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [  2,   5,  33, ...,   0,   0,   0],\n",
              "       [  2,   5,  33, ...,  13, 169,   3],\n",
              "       [  2,   5, 372, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tf.keras.preprocessing.sequence.pad_sequences()\n",
        "###maxlen 파라미터를 사용해 15 초과의 가사들을 잘라냈습니다.\n",
        "###그래서 전체 토큰 개수에는 변화가 없었습니다."
      ],
      "metadata": {
        "id": "xqnP0J18lLE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor[:5,:17])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuzJ6iYLIC5o",
        "outputId": "7002f4e9-f1b7-4a12-f7be-fe105df76742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  306   28   99 4814    3    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2  221   13   85  226    6  115    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   24   17 1087 2353    3    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [ 629    6 2400   43    5  159   15 2762    7   31    6 5633    4   51\n",
            "     3]\n",
            " [   2    5   37   15  163   86 1024   21   73    3    0    0    0    0\n",
            "     0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "  print(idx,':',tokenizer.index_word[idx])\n",
        "  if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxBtKbqaPrxR",
        "outputId": "fd6ecbc7-221f-405c-c67a-48a0f5a61e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scr_input = tensor[:,:-1]\n",
        "tgt_input = tensor[:,1:]\n",
        "\n",
        "print(scr_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ss4YQqTXgrv",
        "outputId": "74d5c1cb-a813-4f6e-8a30-823bcbe03dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  306   28   99 4814    3    0    0    0    0    0    0    0    0]\n",
            "[ 306   28   99 4814    3    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(scr_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(scr_input)//BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((scr_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJRzR_TfcOHA",
        "outputId": "ddfc5d01-9e26-4919-d8e8-fac6c4706d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#enc_train, enc_val, dec_train, dec_val = train_test_split(scr_input,tgt_input,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "mZsbP91BQTyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('Source Train:', enc_train.shape)\n",
        "#print('target Train:', dec_train.shape)"
      ],
      "metadata": {
        "id": "lwV28me7SQL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "    self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "    self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "    self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x):\n",
        "    out = self.embedding(x)\n",
        "    out = self.rnn_1(out)\n",
        "    out = self.rnn_2(out)\n",
        "    out = self.linear(out)\n",
        "\n",
        "    return out\n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "RSNbZNCUTF_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "model.compile(loss=loss,optimizer=optimizer)\n",
        "model.fit(dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scU9uHyeYyQL",
        "outputId": "c16f2e09-8bd6-4299-b533-47697a1daf0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "687/687 [==============================] - 116s 158ms/step - loss: 3.6073\n",
            "Epoch 2/10\n",
            "687/687 [==============================] - 116s 169ms/step - loss: 3.1352\n",
            "Epoch 3/10\n",
            "687/687 [==============================] - 118s 172ms/step - loss: 2.9420\n",
            "Epoch 4/10\n",
            "687/687 [==============================] - 119s 174ms/step - loss: 2.7870\n",
            "Epoch 5/10\n",
            "687/687 [==============================] - 120s 174ms/step - loss: 2.6506\n",
            "Epoch 6/10\n",
            "687/687 [==============================] - 120s 175ms/step - loss: 2.5271\n",
            "Epoch 7/10\n",
            "687/687 [==============================] - 120s 174ms/step - loss: 2.4127\n",
            "Epoch 8/10\n",
            "687/687 [==============================] - 120s 175ms/step - loss: 2.3059\n",
            "Epoch 9/10\n",
            "687/687 [==============================] - 120s 175ms/step - loss: 2.2046\n",
            "Epoch 10/10\n",
            "687/687 [==============================] - 120s 174ms/step - loss: 2.1070\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f939bca2e10>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model,tokenizer, init_sentence='<start>',max_len=20):\n",
        "  test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "  test_tensor = tf.convert_to_tensor(test_input,dtype=tf.int64)\n",
        "  end_token = tokenizer.word_index['<end>']\n",
        "\n",
        "  while True:\n",
        "    predict = model(test_tensor)\n",
        "    predict_word = tf.argmax(tf.nn.softmax(predict,axis=-1), axis=-1)[:,-1]\n",
        "    test_tensor = tf.concat([test_tensor,tf.expand_dims(predict_word,axis=0)], axis=-1)\n",
        "    if predict_word.numpy()[0] == end_token: break\n",
        "    if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "  generated = ''\n",
        "  for word_index in test_tensor[0].numpy():\n",
        "    generated += tokenizer.index_word[word_index] + ' '\n",
        "\n",
        "  return generated"
      ],
      "metadata": {
        "id": "n7EeBAAhiM9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence='<start> my')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8_zYVMRQiQCc",
        "outputId": "569cc064-2479-49e5-c92a-9b2b52c286eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> my chick bad , badder than yours <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 매우 꼼꼼하고 이해하기 쉽게 설명되어 있어 자연어처리에 조금 가까워질 수 있었습니다.\n",
        "### Embedding의 기능이 제일 신기했습니다.\n",
        "\n",
        "### 마지막에 생성된 가사는 칸예나 에미넴 같은 힙합러의 데이터에서 나왔나 추측해 봅니다."
      ],
      "metadata": {
        "id": "L5XtnfH0hbEO"
      }
    }
  ]
}